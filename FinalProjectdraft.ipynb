{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "383e21ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4fb006d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/vedant/opt/anaconda3/lib/python3.9/site-packages (4.35.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/vedant/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: requests in /Users/vedant/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/vedant/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/vedant/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/vedant/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/vedant/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vedant/opt/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /Users/vedant/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/vedant/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/vedant/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/vedant/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/vedant/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/vedant/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/vedant/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/vedant/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vedant/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vedant/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae9d18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vedant/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79f89bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_bbc_sport():\n",
    "    url = 'https://www.bbc.com/sport'\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract all div elements with type \"article\"\n",
    "    article_elements = soup.find_all('div', {'type': 'article'})\n",
    "\n",
    "    # Extract links, headlines, and game names\n",
    "    data = []\n",
    "    for article_element in article_elements:\n",
    "        link_element = article_element.find('a')\n",
    "        if link_element:\n",
    "            link = link_element['href']\n",
    "            full_link = f'http://bbc.com{link}'\n",
    "\n",
    "            headline_element = article_element.find('p')\n",
    "            headline = headline_element.get_text(strip=True) if headline_element else \"\"\n",
    "\n",
    "            # Extract the sport name from the href\n",
    "            sport_name = link.split('/')[-2]\n",
    "\n",
    "            data.append({'sport': sport_name, 'headline': headline, 'weblink': full_link})\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Filter out news articles\n",
    "    df = df[df['sport'] != 'news']\n",
    "\n",
    "    # Categorize Africa-related articles as \"African Sports\"\n",
    "    df['sport'] = df['sport'].apply(lambda x: 'african-sports' if x == 'africa' else x)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85059cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_articles_by_weblinks(df, sport):\n",
    "    sport_articles = df[df['sport'] == sport]\n",
    "    weblinks = sport_articles['weblink'].tolist()\n",
    "\n",
    "    # Create a new DataFrame to store the articles\n",
    "    articles_data = {'title': [], 'article': []}\n",
    "\n",
    "    for weblink in weblinks:\n",
    "        # Send a GET request to the weblink\n",
    "        response = requests.get(weblink)\n",
    "\n",
    "        # Parse the HTML content of the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all p elements with class containing \"$paragraph\" and data-reactid containing \"$paragraph\"\n",
    "        paragraphs = soup.find_all('p', attrs={\"data-reactid\": lambda value: value and \"$paragraph\" in value})\n",
    "\n",
    "        # Extract and store text from each matching p element\n",
    "        article_text = \"\\n\".join(paragraph.get_text(strip=True) for paragraph in paragraphs)\n",
    "        \n",
    "        # Extract the title from the original DataFrame based on the weblink\n",
    "        headline = df[df['weblink'] == weblink]['headline'].iloc[0]\n",
    "\n",
    "        articles_data['title'].append(headline)\n",
    "        articles_data['article'].append(article_text)\n",
    "\n",
    "    # Create a new DataFrame from the collected data\n",
    "    articles_df = pd.DataFrame(articles_data)\n",
    "\n",
    "    return articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3648f4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return analysis.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33ed07cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newsAnalysis(dataDiX):\n",
    "    sent = []\n",
    "    sentVal = []\n",
    "\n",
    "    for news in list(dataDiX.values()):\n",
    "        qq = get_sentiment(news)\n",
    "        sentVal.append(qq)\n",
    "        sentiment = find_Sentiment(qq)\n",
    "        sent.append(sentiment)\n",
    "\n",
    "    return sent, sentVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baad8754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_Sentiment(polarity):\n",
    "    if polarity > 0:\n",
    "        return \"Positive\"\n",
    "    elif polarity < 0:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77e07ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overall_sentiment(article_text):\n",
    "\n",
    "    paragraphs = article_text.split('\\n')\n",
    "    paragraph_sentiments = [get_sentiment(paragraph) for paragraph in paragraphs if paragraph.strip()]\n",
    "    \n",
    "    if paragraph_sentiments:\n",
    "        overall_sentiment = sum(paragraph_sentiments) / len(paragraph_sentiments)\n",
    "        return overall_sentiment\n",
    "    else:\n",
    "        return 0 #if no paraghraph found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfc76f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_bert_model():\n",
    "    # Load pre-trained BERT model and tokenizer\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afb5d783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_summarization_bert(text, model, tokenizer, num_paragraphs=2):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Forward pass through the model\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Get the hidden states from the model output\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # Calculate the mean of the embeddings for each token to obtain a sentence representation\n",
    "    sentence_embeddings = torch.mean(hidden_states, dim=1)\n",
    "\n",
    "    # Summarization: Select a subset of sentences based on some criteria (e.g., sentence importance)\n",
    "\n",
    "    # For example, let's select the top 3 sentences based on the first token's attention weight\n",
    "    #attention_weights = outputs.attentions[0][0][0]  # Get attention weights for the first token\n",
    "    importance_scores = sentence_embeddings[:, 0]\n",
    "    top_sentences_indices = importance_scores.argsort(descending=True)[:3]  # Select the indices of the top 3 sentences\n",
    "\n",
    "    # Extract the top sentences from the original text\n",
    "    top_sentences = [tokenizer.decode(inputs.input_ids[0][idx].item()) for idx in top_sentences_indices]\n",
    "\n",
    "    return top_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfff9e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Sports News Chatbot!\n",
      "Fetching the latest sports news...\n",
      "\n",
      "Available sports categories:\n",
      "1. Football\n",
      "2. Sport\n",
      "3. Tennis\n",
      "4. African-sports\n",
      "5. Snooker\n",
      "6. Rugby-union\n",
      "7. Cricket\n",
      "8. Basketball\n",
      "9. American-football\n",
      "10. Horse-racing\n",
      "11. Rugby-league\n",
      "12. Boxing\n",
      "13. Ice-hockey\n",
      "\n",
      "Enter the number of the sport category you are interested in (type 'exit' to quit): 1\n",
      "\n",
      "Selected Articles with Sentiment Analysis:\n",
      "\n",
      "1.'Wales must settle for play-offs after Turkey draw' : Positive | Sentiment Value: 0.09\n",
      "\n",
      "2.'Croatia secure final automatic Euro 2024 spot' : Positive | Sentiment Value: 0.07\n",
      "\n",
      "3.'Newcastle cleared to sign players from PIF-owned Saudi clubs' : Positive | Sentiment Value: 0.02\n",
      "\n",
      "4.'Germany lose in Austria as Nagelsmann woe continues' : Negative | Sentiment Value: -0.01\n",
      "\n",
      "5.'France's perfect qualifying ended by Greece draw' : Positive | Sentiment Value: 0.08\n",
      "\n",
      "6.'Thousands of fans watch Australia beat Palestine' : Positive | Sentiment Value: 0.13\n",
      "\n",
      "7.'Mead returns to England squad after year out' : Positive | Sentiment Value: 0.27\n",
      "\n",
      "8.'Euro 2024: Who is through, in the play-offs & not going?' : Positive | Sentiment Value: 0.13\n",
      "\n",
      "9.'Republic held by New Zealand as McClean retires' : Positive | Sentiment Value: 0.02\n",
      "\n",
      "10.'Which stars will not be at Euro 2024?' : Positive | Sentiment Value: 0.11\n",
      "\n",
      "11.'Son scores twice as South Korea beat China' : Positive | Sentiment Value: 0.15\n",
      "\n",
      "12.'Kenny's 'instinct' is he will not get Republic extension' : Positive | Sentiment Value: 0.08\n",
      "\n",
      "13.'Ramsdale too good to be a number two - Seaman' : Positive | Sentiment Value: 0.09\n",
      "\n",
      "14.'USA's Dest says red card was 'selfish and immature'' : Positive | Sentiment Value: 0.05\n",
      "\n",
      "15.'San Marino score in third straight match for first time' : Positive | Sentiment Value: 0.16\n",
      "\n",
      "16.'Name the countries with the longest scoring runs' : Positive | Sentiment Value: 0.09\n",
      "\n",
      "17.'FA looking for resolution over trans player dispute' : Positive | Sentiment Value: 0.13\n",
      "\n",
      "18.'What will 'marginal gains' guru Brailsford bring to Man Utd?' : Positive | Sentiment Value: 0.15\n",
      "\n",
      "19.'A rivalry heated by history - the significance of Morocco's win over Spain' : Positive | Sentiment Value: 0.05\n",
      "\n",
      "20.'Bags of urine, cans of Bud and the team that saved US soccer' : Positive | Sentiment Value: 0.06\n",
      "\n",
      "Enter the number of the headline to read the article (type 'back' to go back): 1\n"
     ]
    }
   ],
   "source": [
    "def chat_bot():\n",
    "    print(\"Welcome to the Sports News Chatbot!\")\n",
    "    print(\"Fetching the latest sports news...\\n\")\n",
    "\n",
    "    # Get the latest sports news\n",
    "    sports_news_df = scrape_bbc_sport()\n",
    "\n",
    "    # Display available sports categories\n",
    "    sports_categories = sports_news_df['sport'].unique()\n",
    "    print(\"Available sports categories:\")\n",
    "    for idx, category in enumerate(sports_categories, start=1):\n",
    "        print(f\"{idx}. {category.capitalize()}\")\n",
    "\n",
    "    # User interaction loop\n",
    "    while True:\n",
    "        user_input = input(\"\\nEnter the number of the sport category you are interested in (type 'exit' to quit): \")\n",
    "\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye! Have a great day.\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            selection_index = int(user_input)\n",
    "            if 1 <= selection_index <= len(sports_categories):\n",
    "                selected_category = sports_categories[selection_index - 1]\n",
    "                selected_articles = retrieve_articles_by_weblinks(sports_news_df, selected_category)\n",
    "\n",
    "                # Extract titles and stories as a dictionary\n",
    "                titles_and_stories = dict(zip(selected_articles['title'], selected_articles['article']))\n",
    "\n",
    "                # Perform sentiment analysis\n",
    "                sentiment_labels, sentiment_values = newsAnalysis(titles_and_stories)\n",
    "\n",
    "                print(f\"\\nSelected Articles with Sentiment Analysis:\")\n",
    "                for idx, (title, sentiment_label, sentiment_value) in enumerate(zip(selected_articles['title'], sentiment_labels, sentiment_values), start=1):\n",
    "                    print(f\"\\n{idx}.'{title}' : {sentiment_label} | Sentiment Value: {sentiment_value:.2f}\")\n",
    "\n",
    "                headline_index = input(\"\\nEnter the number of the headline to read the article (type 'back' to go back): \")\n",
    "\n",
    "                if headline_index.lower() == 'back':\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    headline_index = int(headline_index) - 1\n",
    "                    selected_headline = selected_articles.iloc[headline_index]\n",
    "                   \n",
    "                # Integrate BERT for sentence summarization\n",
    "                    bert_model, bert_tokenizer = initialize_bert_model()\n",
    "                    summary_sentences = sentence_summarization_bert(selected_headline['article'], bert_model, bert_tokenizer, num_paragraphs=2)\n",
    "                    \n",
    "                    print(\"\\nSelected Article Summary Sentences:\")\n",
    "                    for sentence in summary_sentences:\n",
    "                        print(sentence)\n",
    "                        \n",
    "                    print(f\"\\nReading article for '{selected_headline['title']}':\\n\")\n",
    "                    #print(f\"\\nSummary for '{selected_headline['title']}':\\n{article_summary}\")\n",
    "\n",
    "                except (ValueError, IndexError):\n",
    "                    print(\"Invalid index. Please enter a valid headline number.\")\n",
    "            else:\n",
    "                print(\"Invalid index. Please select a valid sports category.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a number or 'exit'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat_bot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400aa99c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
